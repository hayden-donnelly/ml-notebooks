{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ef0f36",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "This is my implementation of a Transfomer language model loosely follwing Andrej Karpathy's video lecture [Let's Build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1866s).\n",
    "\n",
    "Some key differences between his lecture and this notebook:\n",
    "- This notebook uses Keras/Tensorflow instead of PyTorch\n",
    "- This notebook uses Keras' MultiHeadAttention and LayerNormalization layers instead of implementing them from scratch\n",
    "- This notebook uses TikToken for token embeddings (TODO)\n",
    "\n",
    "ToDo List:\n",
    "- Implement TikToken\n",
    "- Write custom train loop\n",
    "- Validation\n",
    "- Dropout\n",
    "- Read transformer lit and improve architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47820fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbef3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0362f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261c0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa254ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n",
      " 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n",
      " 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n",
      " 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n",
      " 56 57 58  6  1 63 53 59], shape=(200,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant(encode(text))\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f2ea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5a526f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "batch_size = 64\n",
    "embed_dim = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.experimental.numpy.random.randint(low = 0, high = len(data) - block_size, size = batch_size)\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack(tf.one_hot(indices=[data[i+1:i+block_size+1] for i in ix], depth=vocab_size))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36b8ad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "245b4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(depth):\n",
    "    causal_mask = np.tril(np.ones((block_size, block_size)), 0)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(block_size,))\n",
    "    # Token embedding.\n",
    "    tok_emb = tf.keras.layers.Embedding(vocab_size, vocab_size)(inputs)\n",
    "    # Positional embedding.\n",
    "    pos_emb_layer = tf.keras.layers.Embedding(block_size, vocab_size)\n",
    "    pos_emb = pos_emb_layer(np.arange(block_size))\n",
    "    x = tok_emb + pos_emb\n",
    "\n",
    "    for _ in range(depth):\n",
    "        # Attention\n",
    "        mha = tf.keras.layers.MultiHeadAttention(num_heads=6, \n",
    "                                                 key_dim=vocab_size, \n",
    "                                                 value_dim=vocab_size)\n",
    "\n",
    "        att = mha(query=x, value=x, attention_mask=causal_mask)\n",
    "        x = tf.keras.layers.LayerNormalization()(x + att)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff1 = tf.keras.layers.Dense(units=vocab_size, activation='relu')(x)\n",
    "        ff2 = tf.keras.layers.Dense(units=vocab_size)(ff1)\n",
    "        x = tf.keras.layers.LayerNormalization()(x + ff2)\n",
    "\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "968fe952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_string, num_new_tokens):\n",
    "    s = input_string\n",
    "    x = encode(input_string)\n",
    "    for _ in range(num_new_tokens):\n",
    "        out = model.predict(tf.constant([x]))\n",
    "        next_token = np.random.choice(np.arange(0, vocab_size), p=out[0][-1])\n",
    "        x.append(next_token)\n",
    "        x = x[1:]\n",
    "        s += decode([next_token])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cef8978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_steps):\n",
    "    for step in range(train_steps):\n",
    "        xb, yb = get_batch('train')\n",
    "        model.fit(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4698878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e316339",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = generate(model, text[:block_size], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15820c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "Have me; sir, his is a ster?\n",
      "\n",
      "PAULINA:\n",
      "I'll be contience too, Hastile Coriolanus\n",
      "As men: as I have no what never, your majesty\n",
      "That go your bents there your saves,\n",
      "And that was to have you. Come! Which indeed you?\n",
      "\n",
      "CORIOLANUS:\n",
      "Then yield say there\n",
      "all all it full with sea roughs throw ask\n",
      "Thormening away from my fashiling grief;\n",
      "Who the gods so, conveniency, seance, and,\n",
      "Which iat they not, and actions strangely\n",
      "Have the herself proppert her: here say\n",
      "Is were this Gaster one, lovethour shall\n",
      "The tears men boan to-day poind or spart.\n",
      "\n",
      "First Murderer:\n",
      "When it, tucketh you are copens\n",
      "For callenty many head?\n",
      "\n",
      "LUCIO:\n",
      "Ten 'made me all dirot deliver and of a back;\n",
      "The vpervel with likecowiens\n",
      "to give you are giverse, being my fleave-by in\n",
      "Unto thought this word sheep my love.\n",
      "\n",
      "PAULIT:\n",
      "Wout years would have till him memberlelss;\n",
      "In do what see his bless lend, and thou\n",
      "paens of fair, awake to where?\n",
      "\n",
      "Bone,\n",
      "And keep with the man fairer therer of my valota?\n",
      "Think therefore harm me injured?\n",
      "\n",
      "DUKE \n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaccd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
