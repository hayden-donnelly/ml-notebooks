{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ef0f36",
   "metadata": {},
   "source": [
    "This is my implementation of a Transfomer language model loosely follwing Andrej Karpathy's video lecture [Let's Build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1866s).\n",
    "\n",
    "Some key differences between his lecture and this notebook:\n",
    "- This notebook uses Keras/Tensorflow instead of PyTorch\n",
    "- This notebook uses Keras' MultiHeadAttention and LayerNormalization layers instead of implementing them from scratch\n",
    "- This notebook uses TikToken for token embeddings (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47820fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbef3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0362f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261c0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa254ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n",
      " 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n",
      " 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n",
      " 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n",
      " 56 57 58  6  1 63 53 59], shape=(200,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant(encode(text))\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2ea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.9 * len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a526f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = tf.experimental.numpy.random.randint(low = 0, high = len(data) - block_size, size = batch_size)\n",
    "    x = tf.stack([data[i:i+block_size] for i in ix])\n",
    "    y = tf.stack(tf.one_hot(indices=[data[i+1:i+block_size+1] for i in ix], depth=vocab_size))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "245b4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(depth):\n",
    "    causal_mask = np.tril(np.ones((block_size, block_size)), 0)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(block_size,))\n",
    "    x = tf.keras.layers.Embedding(vocab_size, vocab_size)(inputs)\n",
    "\n",
    "    for _ in range(depth):\n",
    "        # Attention\n",
    "        mha = tf.keras.layers.MultiHeadAttention(num_heads=1, \n",
    "                                                 key_dim=vocab_size, \n",
    "                                                 value_dim=vocab_size)\n",
    "\n",
    "        att = mha(query=x, value=x, attention_mask=causal_mask)\n",
    "        x = tf.keras.layers.LayerNormalization()(x + att)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff1 = tf.keras.layers.Dense(units=vocab_size, activation='relu')(x)\n",
    "        ff2 = tf.keras.layers.Dense(units=vocab_size)(ff1)\n",
    "        x = tf.keras.layers.LayerNormalization()(x + ff2)\n",
    "\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968fe952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_string, num_new_tokens):\n",
    "    s = input_string\n",
    "    x = encode(input_string)\n",
    "    for _ in range(num_new_tokens):\n",
    "        out = model.predict(tf.constant([x]))\n",
    "        next_token = np.random.choice(np.arange(0, vocab_size), p=out[0][-1])\n",
    "        x.append(next_token)\n",
    "        x = x[1:]\n",
    "        s += decode([next_token])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef8978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_steps):\n",
    "    for step in range(train_steps):\n",
    "        xb, yb = get_batch('train')\n",
    "        model.fit(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4698878",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model(depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a6f1d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step - loss: 4.7556\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.2429\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.9281\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.7120\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.5341\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.4619\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4147\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3899\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3033\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2775\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.2249\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.1958\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1970\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.1393\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1397\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0924\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1119\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0427\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0466\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0063\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9989\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8565\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8708\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9079\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8955\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8278\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8817\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8301\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8340\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7870\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8141\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7697\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7619\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8129\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7946\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7610\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7704\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7089\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.7571\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7665\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.7274\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.7381\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.7169\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7125\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6503\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6861\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6807\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.7137\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.7228\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6855\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6869\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6851\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.7106\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6773\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6715\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6675\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6614\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.6705\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.6816\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6671\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6355\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6440\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6268\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5922\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6241\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6714\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6265\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5883\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6381\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6191\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6547\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6528\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6285\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6513\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6338\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6095\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.5984\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5990\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.5828\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6060\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6291\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6063\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6410\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6385\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5986\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6233\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.5968\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.5998\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5639\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5898\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6083\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6321\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.5976\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5700\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.5881\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5530\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6173\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5855\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5982\n"
     ]
    }
   ],
   "source": [
    "train(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e316339",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "Al?\n",
      "SAqundl merowet ollangh, Zmyce yonoasinXE UL3 st&enemnt,MICe FZEd hy, owamal ngh th! mXEgfNY:\n",
      "V,\n",
      "S\n"
     ]
    }
   ],
   "source": [
    "output = generate(model, text[:64], 100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15820c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
